{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Tensor Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Author: Seungjae Ryan Lee (seungjaeryanlee at gmail dot com)\n",
    "Second Author: Ki Hyun Kim (nlp.with.deep.learning at gmail dot com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    NOTE: This corresponds to <a href=\"https://www.youtube.com/watch?v=ZYX0FaqUeN4&t=23s&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=25\">Lab 8 of Deep Learning Zero to All Season 1 for TensorFlow</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `pip install -r requirements.txt` in terminal to install all required Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope that you are familiar with `numpy` and basic linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Array with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "t = np.array([0., 1., 2., 3., 4., 5., 6.])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  of t:  1\n",
      "Shape of t:  (7,)\n"
     ]
    }
   ],
   "source": [
    "print('Rank  of t: ', t.ndim)\n",
    "print('Shape of t: ', t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t[0] t[1] t[-1] =  0.0 1.0 6.0\n",
      "t[2:5] t[4:-1]  =  [2. 3. 4.] [4. 5.]\n",
      "t[:2] t[3:]     =  [0. 1.] [3. 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "print('t[0] t[1] t[-1] = ', t[0], t[1], t[-1]) # Element\n",
    "print('t[2:5] t[4:-1]  = ', t[2:5], t[4:-1])   # Slicing\n",
    "print('t[:2] t[3:]     = ', t[:2], t[3:])      # Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Array with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]\n",
      " [ 7.  8.  9.]\n",
      " [10. 11. 12.]]\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  of t:  2\n",
      "Shape of t:  (4, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Rank  of t: ', t.ndim)\n",
    "print('Shape of t: ', t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch is like NumPy (but better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Array with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "tensor(0.) tensor(1.) tensor(6.)\n",
      "tensor([2., 3., 4.]) tensor([4., 5.])\n",
      "tensor([0., 1.]) tensor([3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "print(t.dim())  # rank\n",
    "print(t.shape)  # shape\n",
    "print(t.size()) # shape\n",
    "print(t[0], t[1], t[-1])  # Element\n",
    "print(t[2:5], t[4:-1])    # Slicing\n",
    "print(t[:2], t[3:])       # Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Array with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1., 2., 3.],\n",
    "                       [4., 5., 6.],\n",
    "                       [7., 8., 9.],\n",
    "                       [10., 11., 12.]\n",
    "                      ])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([4, 3])\n",
      "tensor([ 2.,  5.,  8., 11.])\n",
      "torch.Size([4])\n",
      "tensor([[ 1.,  2.],\n",
      "        [ 4.,  5.],\n",
      "        [ 7.,  8.],\n",
      "        [10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "print(t.dim())  # rank\n",
    "print(t.size()) # shape\n",
    "print(t[:, 1])\n",
    "print(t[:, 1].size())\n",
    "print(t[:, :-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape, Rank, Axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.FloatTensor([[[[1, 2, 3, 4],\n",
    "                         [5, 6, 7, 8],\n",
    "                         [9, 10, 11, 12]],\n",
    "                       [[13, 14, 15, 16],\n",
    "                        [17, 18, 19, 20],\n",
    "                        [21, 22, 23, 24]]\n",
    "                       ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(t.dim())  # rank  = 4\n",
    "print(t.size()) # shape = (1, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequently Used Operations in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [mul](https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul) vs. [matmul](https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Mul vs Matmul\n",
      "-------------\n",
      "tensor.matmul (a,b) is matrix mulitplication\n",
      "m1: tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "m2: tensor([[1.],\n",
      "        [2.]])\n",
      "Shape of Matrix 1:  torch.Size([2, 2])\n",
      "Shape of Matrix 2:  torch.Size([2, 1])\n",
      "m1.matmul(m2):\n",
      " tensor([[ 5.],\n",
      "        [11.]])\n",
      "====================\n",
      "a*b or tensor.mul(b) is element-wise mulitplication\n",
      "m1: tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "m2: tensor([[1.],\n",
      "        [2.]])\n",
      "Shape of Matrix 1:  torch.Size([2, 2])\n",
      "Shape of Matrix 2:  torch.Size([2, 1])\n",
      "m1*m2:\n",
      " tensor([[1., 2.],\n",
      "        [6., 8.]])\n",
      "m1.mul(m2):\n",
      " tensor([[1., 2.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('-------------')\n",
    "print('Mul vs Matmul')\n",
    "print('-------------')\n",
    "print(\"tensor.matmul (a,b) is matrix mulitplication\")\n",
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "print(f\"m1: {m1}\")\n",
    "print(f\"m2: {m2}\")\n",
    "print('Shape of Matrix 1: ', m1.shape) # 2 x 2\n",
    "print('Shape of Matrix 2: ', m2.shape) # 2 x 1\n",
    "\n",
    "print(f\"m1.matmul(m2):\\n {m1.matmul(m2)}\") # 2 x 1\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(\"a*b or tensor.mul(b) is element-wise mulitplication\")\n",
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "print(f\"m1: {m1}\")\n",
    "print(f\"m2: {m2}\")\n",
    "print('Shape of Matrix 1: ', m1.shape) # 2 x 2\n",
    "print('Shape of Matrix 2: ', m2.shape) # 2 x 1\n",
    "print(f\"m1*m2:\\n {m1 * m2}\") # 2 x 2\n",
    "print(f\"m1.mul(m2):\\n {m1.mul(m2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Broadcasting](https://pytorch.org/docs/stable/notes/broadcasting.html?highlight=broadcasting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    Carelessly using broadcasting can lead to code hard to debug.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# Same shape\n",
    "m1 = torch.FloatTensor([[3, 3]])\n",
    "m2 = torch.FloatTensor([[2, 2]])\n",
    "print(m1 + m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# Vector + scalar\n",
    "m1 = torch.FloatTensor([[1, 2]])\n",
    "m2 = torch.FloatTensor([3]) # 3 -> [[3, 3]]\n",
    "print(m1 + m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 5.],\n",
      "        [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# 2 x 1 Vector + 1 x 2 Vector\n",
    "m1 = torch.FloatTensor([[1, 2]])  # [[1,2]] ==> [ [1,2], p1,2] }\n",
    "m2 = torch.FloatTensor([[3], [4]])# [[3],[4]] ==> [ [3,3], [4,4]]\n",
    "print(m1 + m2)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [mean](https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5000)\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([1, 2])\n",
    "print(t.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can only calculate the mean of floating types. Got Long instead.\n"
     ]
    }
   ],
   "source": [
    "# Can't use mean() on integers\n",
    "t = torch.LongTensor([1, 2])\n",
    "try:\n",
    "    print(t.mean())\n",
    "except Exception as exc:\n",
    "    print(exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `t.mean` for higher rank tensors to get mean of all elements, or mean by particular dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5000)\n",
      "tensor([2., 3.])\n",
      "tensor([1.5000, 3.5000])\n",
      "tensor([1.5000, 3.5000])\n"
     ]
    }
   ],
   "source": [
    "print(t.mean())       # element-wise maen : (1+2+3+4)/2\n",
    "print(t.mean(dim=0))  # row-wise mean  : [(1+3)/2, (2+4)/2]\n",
    "print(t.mean(dim=1))  # colum-wis mean : [(1+2)/2, (3+4)/2]\n",
    "print(t.mean(dim=-1)) # colum-wis mean: [(1+2)/2, (3+4)/2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [sum](https://pytorch.org/docs/stable/tensors.html?highlight=sum#torch.Tensor.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "tensor([4., 6.])\n",
      "tensor([3., 7.])\n",
      "tensor([3., 7.])\n"
     ]
    }
   ],
   "source": [
    "print(t.sum())       # element-wise sum : 1+2+3+4\n",
    "print(t.sum(dim=0))  # row-wise sum : [ 1+3, 2+4]\n",
    "print(t.sum(dim=1))  # colum-wis sum: [(1+2), (3+4)]\n",
    "print(t.sum(dim=-1)) # colum-wis sum: [(1+2), (3+4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [max](https://pytorch.org/docs/stable/generated/torch.max.html) and [argmax](https://pytorch.org/docs/stable/tensors.html?highlight=argmax#torch.Tensor.argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [torch.max()](https://pytorch.org/docs/stable/generated/torch.max.html) operator returns one value if it is called without an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "print(t.max()) # Returns one value: max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [torch.Tensor.max()](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.max) operator returns 2 values when called with dimension specified. The first value is the maximum value, and the second value is the [torch.Tensor.argmax()](https://pytorch.org/docs/stable/tensors.html?highlight=argmax#torch.Tensor.argmax): the index of the element with maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([3., 4.]),\n",
      "indices=tensor([1, 1]))\n",
      "Max:  tensor([3., 4.])\n",
      "Argmax:  tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(t.max(dim=0)) # Returns two values: max and argmax\n",
    "print('Max: ', t.max(dim=0)[0])    # first element of returned list is max value\n",
    "print('Argmax: ', t.max(dim=0)[1]) # seocond element of returned list is index of cell that contains max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([2., 4.]),\n",
      "indices=tensor([1, 1]))\n",
      "torch.return_types.max(\n",
      "values=tensor([2., 4.]),\n",
      "indices=tensor([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(t.max(dim=1))\n",
    "print(t.max(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [View](https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    This is a function hard to master, but is very useful!\n",
    "</div>\n",
    "\n",
    "[torch.Tensor.view](https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view)  \n",
    "view(*shape) → Tensor\n",
    "* Returns a new tensor with the same data as the self tensor but of a different shape.\n",
    "\n",
    "The returned tensor shares the same data and must have the same number of elements, but may have a different size. For a tensor to be viewed, the new view size must be compatible with its original size and stride, i.e., each new view dimension must either be a subspace of an original dimension, or only span across original dimensions $d, d+1, \\dots, d+k$ that satisfy the following contiguity-like condition that $\\forall i = d, \\dots, d+k-1$ ,\n",
    "$$\n",
    "\\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1]\n",
    "$$\n",
    "\n",
    "When it is unclear whether a [view()](https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view) can be performed, it is advisable to use [reshape()](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape), which returns a view if the shapes are compatible, and copies (equivalent to calling [contiguous()](https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.contiguous) otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "t = np.array([\n",
    "               [[0, 1, 2],\n",
    "               [3, 4, 5]\n",
    "              ],\n",
    "              [\n",
    "               [6, 7, 8],\n",
    "               [9, 10, 11]\n",
    "              ]\n",
    "              ])\n",
    "ft = torch.FloatTensor(t)  # convert np.array to torch.FloatTensor\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view([-1, 3]))\n",
    "print(ft.view([-1, 3]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.]],\n",
      "\n",
      "        [[ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11.]]])\n",
      "torch.Size([4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view([-1, 1, 3]))\n",
    "print(ft.view([-1, 1, 3]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Squeeze](https://pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze)\n",
    "\n",
    "squeeze(dim=None) → Tensor\n",
    "\n",
    "Returns a tensor with all the dimensions of input of size 1 removed.\n",
    "\n",
    "For example, if input is of shape: $(A \\times 1 \\times B \\times C \\times 1 \\times D)$ then the out tensor will be of shape: $(A \\times B \\times C \\times D)$.\n",
    "\n",
    "When ```dim``` is given, a squeeze operation is done only in the given dimension. If input is of shape: $(A \\times 1 \\times B)$, squeeze(input, 0) leaves the tensor unchanged, but squeeze(input, 1) will squeeze the tensor to the shape $(A \\times B)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.FloatTensor([[0], [1], [2]])\n",
    "print(ft)\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.squeeze())\n",
    "print(ft.squeeze().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze)  \n",
    "\n",
    "torch.unsqueeze(input, dim) → Tensor\n",
    "\n",
    "Returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "\n",
    "The returned tensor shares the same underlying data with this tensor.\n",
    "\n",
    "A ```dim``` value within the range ```[-input.dim() - 1, input.dim() + 1)``` can be used. Negative ```dim``` will correspond to unsqueeze() applied at ```dim = dim + input.dim() + 1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.Tensor([0, 1, 2])\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(0))\n",
    "print(ft.unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view(1, -1))\n",
    "print(ft.view(1, -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(1))\n",
    "print(ft.unsqueeze(1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(-1))\n",
    "print(ft.unsqueeze(-1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [scatter](https://pytorch.org/docs/stable/tensors.html?highlight=scatter#torch.Tensor.scatter_) (for one-hot encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    Scatter is a very flexible function. We only discuss how to use it to get a one-hot encoding of indices.\n",
    "</div>\n",
    "\n",
    "scatter_(dim, index, src, reduce=None) → Tensor\n",
    "\n",
    "Writes all values from the tensor ```src``` into ```self``` at the indices specified in the ```index``` tensor. For each value in ```src```, its output index is specified by its index in ```src``` for ```dimension != dim``` and by the corresponding value in ```index``` for ```dimension = dim```.\n",
    "\n",
    "For a 3-D tensor, self is updated as:\n",
    "````\n",
    "self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\n",
    "self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\n",
    "self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\n",
    "````\n",
    "This is the reverse operation of the manner described in [gather()](https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0]])\n"
     ]
    }
   ],
   "source": [
    "# index list for change the values to one in each row\n",
    "lt = torch.LongTensor([[0], [1], [2], [0]])\n",
    "print(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "one_hot = torch.zeros(4, 3) # batch_size = 4, classes = 3\n",
    "one_hot.scatter_(1, lt, 1) # dim=1, index= lt ??\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "lt = torch.LongTensor([1, 2, 3, 4])\n",
    "print(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "print(lt.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 1], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "bt = torch.ByteTensor([True, False, False, True])\n",
    "print(bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 1])\n",
      "tensor([1., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(bt.long())\n",
    "print(bt.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Concatenation](https://pytorch.org/docs/stable/generated/torch.cat.html?highlight=torch%20tensor%20cat)\n",
    "\n",
    "torch.cat(tensors, dim=0, *, out=None) → Tensor\n",
    "\n",
    "Concatenates the given sequence of ```seq``` tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.\n",
    "\n",
    "[torch.cat()](https://pytorch.org/docs/stable/generated/torch.cat.html?highlight=torch%20tensor%20cat#torch.cat) can be seen as an inverse operation for [torch.split()](https://pytorch.org/docs/stable/generated/torch.split.html#torch.split) and [torch.chunk()](https://pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "y = torch.FloatTensor([[5, 6], [7, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.],\n",
      "        [7., 8.]])\n",
      "tensor([[1., 2., 5., 6.],\n",
      "        [3., 4., 7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat([x, y], dim=0))   # vertical stacking\n",
    "print(torch.cat([x, y], dim=1))   # horizontal stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Stacking](https://pytorch.org/docs/stable/generated/torch.stack.html?highlight=torch%20stack#torch.stack)\n",
    "\n",
    "torch.stack(tensors, dim=0, *, out=None) → Tensor\n",
    "\n",
    "Concatenates a sequence of tensors along a new dimension.\n",
    "\n",
    "All tensors need to be of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([1, 4])\n",
    "y = torch.FloatTensor([2, 5])\n",
    "z = torch.FloatTensor([3, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.stack([x, y, z]))     # vertical stacking\n",
    "print(torch.stack([x, y, z], dim=1))  # horizontal stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ones and Zeros Like\n",
    "\n",
    "[torch.ones_like()](https://pytorch.org/docs/stable/generated/torch.ones_like.html?highlight=ones_like#torch.ones_like)\n",
    "\n",
    "Returns a tensor filled with the scalar value 1, with the same size as ```input```. ```torch.ones_like(input)``` is equivalent to ```torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)```.\n",
    "\n",
    "[torch.zeors_like()](https://pytorch.org/docs/stable/generated/torch.zeros_like.html?highlight=zeros_like)\n",
    "\n",
    "Returns a tensor filled with the scalar value 0, with the same size as ```input```. ```torch.zeros_like(input)``` is equivalent to ```torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [2., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.ones_like(x))\n",
    "print(torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-place Operation\n",
    "\n",
    "[torch.Tensor.mul_()](https://pytorch.org/docs/stable/tensors.html?highlight=torch%20mul_#torch.Tensor.mul) is In-place version of mul().\n",
    "\n",
    "Note that operators by underline are in-place version. i.e, change the data in the Tesor instead of make copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 8.])\n",
      "tensor([1., 4.])\n",
      "tensor([2., 8.])\n",
      "tensor([2., 8.])\n"
     ]
    }
   ],
   "source": [
    "print(x.mul(2.))   \n",
    "print(x)\n",
    "print(x.mul_(2.))  # in-place version mul\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip\n",
    "\n",
    "파이썬 함수인 zip(*iterable)은 동일한 개수로 이루어진 자료형을 묶어 주는 역할을 하는 함수이다.\n",
    "\n",
    "````\n",
    ">>> list(zip([1, 2, 3], [4, 5, 6]))\n",
    "[(1, 4), (2, 5), (3, 6)]\n",
    ">>> list(zip([1, 2, 3], [4, 5, 6], [7, 8, 9]))\n",
    "[(1, 4, 7), (2, 5, 8), (3, 6, 9)]\n",
    ">>> list(zip(\"abc\", \"def\"))\n",
    "[('a', 'd'), ('b', 'e'), ('c', 'f')]\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4\n",
      "2 5\n",
      "3 6\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip([1, 2, 3], [4, 5, 6]):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 7\n",
      "2 5 8\n",
      "3 6 9\n"
     ]
    }
   ],
   "source": [
    "for x, y, z in zip([1, 2, 3], [4, 5, 6], [7, 8, 9]):\n",
    "    print(x, y, z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
